{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1c81fd-4f91-49cf-bc24-88a55d97afc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-26 22:25:21 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 08-26 22:25:35 [config.py:841] This model supports multiple tasks: {'embed', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 08-26 22:25:35 [config.py:1472] Using max model len 131072\n",
      "WARNING 08-26 22:25:35 [arg_utils.py:1577] The model has a long context length (131072). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.\n",
      "WARNING 08-26 22:25:35 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 08-26 22:25:35 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 08-26 22:25:36 [cuda.py:351] Using Flash Attention backend.\n",
      "INFO 08-26 22:25:36 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 08-26 22:25:36 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e9762209b6439daf9cc4348bb8b1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-26 22:25:38 [default_loader.py:272] Loading weights took 1.19 seconds\n",
      "INFO 08-26 22:25:39 [model_runner.py:1255] Model loading took 3.3461 GiB and 1.355255 seconds\n",
      "INFO 08-26 22:25:43 [worker.py:295] Memory profiling takes 3.98 seconds\n",
      "INFO 08-26 22:25:43 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.44GiB) x gpu_memory_utilization (0.90) = 42.69GiB\n",
      "INFO 08-26 22:25:43 [worker.py:295] model weights take 3.35GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 8.07GiB; the rest of the memory reserved for KV Cache is 31.22GiB.\n",
      "INFO 08-26 22:25:43 [executor_base.py:115] # cuda blocks: 73063, # CPU blocks: 9362\n",
      "INFO 08-26 22:25:43 [executor_base.py:120] Maximum concurrency for 131072 tokens per request: 8.92x\n",
      "INFO 08-26 22:25:46 [llm_engine.py:430] init engine (profile, create kv cache, warmup model) took 7.83 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest, VectorConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\"\n",
    ")\n",
    "\n",
    "# Initialize LLM with steering vector capability\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "432e3c0b-253a-409f-8fe3-edf89bf555d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the suffix for newline tokens in the tokenizer\n",
    "target_suffix = \"ĊĊ\"  # \"\\n\\n\" is tokenized as \"ĊĊ\"\n",
    "\n",
    "# Get complete tokenizer vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Find all tokens and their IDs that end with the target suffix\n",
    "# These are the newline tokens we'll apply steering to\n",
    "matching_tokens_ids = [\n",
    "    token_id\n",
    "    for token, token_id in vocab.items()\n",
    "    if isinstance(token, str) and token.endswith(target_suffix)\n",
    "]\n",
    "\n",
    "# Configure steering vector request for SEAL control\n",
    "sv_request = SteerVectorRequest(\n",
    "    # Name and ID for the steering vector\n",
    "    steer_vector_name=\"complex_control\",\n",
    "    steer_vector_id=4,\n",
    "    \n",
    "    # Configure the three steering vectors (execution, reflection, transition)\n",
    "    vector_configs=[\n",
    "        # Execution vector (positive scale to promote execution-like text)\n",
    "        VectorConfig(\n",
    "            path=\"execution_avg_vector.gguf\",\n",
    "            scale=0.5,                            # Positive scale promotes this behavior\n",
    "            target_layers=[20],                   # Apply at layer 20\n",
    "            generate_trigger_tokens=matching_tokens_ids,  # Apply to newline tokens\n",
    "            algorithm=\"direct\",                   # Direct application\n",
    "            normalize=False                       # Do not normalize vectors\n",
    "        ),\n",
    "        \n",
    "        # Reflection vector (negative scale to suppress reflection)\n",
    "        VectorConfig(\n",
    "            path=\"reflection_avg_vector.gguf\",\n",
    "            scale=-0.5,                           # Negative scale suppresses this behavior\n",
    "            target_layers=[20],\n",
    "            generate_trigger_tokens=matching_tokens_ids,\n",
    "            algorithm=\"direct\",\n",
    "            normalize=False\n",
    "        ),\n",
    "        \n",
    "        # Transition vector (negative scale to suppress transitions)\n",
    "        VectorConfig(\n",
    "            path=\"transition_avg_vector.gguf\",\n",
    "            scale=-0.5,                           # Negative scale suppresses this behavior\n",
    "            target_layers=[20],\n",
    "            generate_trigger_tokens=matching_tokens_ids,\n",
    "            algorithm=\"direct\", \n",
    "            normalize=False\n",
    "        ),\n",
    "    ],\n",
    "    \n",
    "    # Additional parameters\n",
    "    debug=False,                        # Don't output debug info\n",
    "    conflict_resolution=\"sequential\"    # Apply vectors in sequence\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e623edf-0aea-4ef3-9859-6215bd35879c",
   "metadata": {},
   "source": [
    "# MATH500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c8079f-369f-4681-b02d-1a5905f41cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems: ['Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\\\theta),$ where $r > 0$ and $0 \\\\le \\\\theta < 2 \\\\pi.$', 'Define\\n\\\\[p = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^2} \\\\quad \\\\text{and} \\\\quad q = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^3}.\\\\]Find a way to write\\n\\\\[\\\\sum_{j = 1}^\\\\infty \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\]in terms of $p$ and $q.$']\n",
      "Answers: ['\\\\left( 3, \\\\frac{\\\\pi}{2} \\\\right)', 'p - q']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"/home/xhl/eval/my_eval/data/math500/test.jsonl\"\n",
    "\n",
    "problems = []\n",
    "answers = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        problems.append(item[\"problem\"])\n",
    "        answers.append(item[\"answer\"])\n",
    "\n",
    "# 看看前两个\n",
    "print(\"Problems:\", problems[:2])\n",
    "print(\"Answers:\", answers[:2])\n",
    "\n",
    "\n",
    "examples = [\"Please reason step by step, and put your final answer within \\\\boxed{}.\\nUser: \" + prompt + \"\\nAssistant: <think>\" for prompt in problems]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974583e4-6e84-492f-b6ed-42d7badaf121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response with SEAL steering\n",
    "example_answers = llm.generate(\n",
    "    examples, \n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=8192,\n",
    "        skip_special_tokens=False,\n",
    "    ), \n",
    "    steer_vector_request=sv_request\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91af7056-7aac-446a-97d5-0cea4a2acad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.784\n"
     ]
    }
   ],
   "source": [
    "from math_verify import parse, verify, LatexExtractionConfig, ExprExtractionConfig\n",
    "outputs = [output.outputs[0].text for output in example_answers]\n",
    "extraction_target = (ExprExtractionConfig(), LatexExtractionConfig())\n",
    "results = []\n",
    "for i, llm_output in enumerate(outputs):\n",
    "    gold = parse(f\"${answers[i]}$\", extraction_config=extraction_target)\n",
    "    answer = parse(llm_output, extraction_config=extraction_target)\n",
    "    result = verify(gold, answer)\n",
    "    results.append(result)\n",
    "accuracy = sum(results) / len(results)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed09dbaa-a6a9-41a4-b128-344908f536e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  3074.668\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\"\n",
    ")\n",
    "length = 0\n",
    "for i in range(len(outputs)):\n",
    "    length += len(tokenizer.tokenize(outputs[i], add_special_tokens=True))\n",
    "print(\"Length: \", length/len(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e8934e-ee16-4959-9a5a-5c55a7c6a9bb",
   "metadata": {},
   "source": [
    "# GSM8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20941347-a40d-4597-8aff-2906f6e92280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems: [\"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\", 'A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?']\n",
      "Answers: ['Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18', 'It takes 2/2=<<2/2=1>>1 bolt of white fiber\\nSo the total amount of fabric is 2+1=<<2+1=3>>3 bolts of fabric\\n#### 3']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"/home/xhl/eval/my_eval/data/gsm8k/test.jsonl\"\n",
    "\n",
    "problems = []\n",
    "answers = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        problems.append(item[\"question\"])\n",
    "        answers.append(item[\"answer\"])\n",
    "\n",
    "# 看看前两个\n",
    "print(\"Problems:\", problems[:2])\n",
    "print(\"Answers:\", answers[:2])\n",
    "\n",
    "\n",
    "examples = [\"Please reason step by step, and put your final answer within \\\\boxed{}.\\nUser: \" + prompt + \"\\nAssistant: <think>\" for prompt in problems]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81643a92-d71a-48c0-a066-baea4ae29c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_answers = llm.generate(\n",
    "    examples, \n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=8192,\n",
    "        skip_special_tokens=False,\n",
    "    ), \n",
    "    steer_vector_request=sv_request\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93431b9-836e-4216-81e4-e57be98a9448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8233510235026535\n"
     ]
    }
   ],
   "source": [
    "from math_verify import parse, verify, LatexExtractionConfig, ExprExtractionConfig\n",
    "outputs = [output.outputs[0].text for output in example_answers]\n",
    "extraction_target = (ExprExtractionConfig(), LatexExtractionConfig())\n",
    "results = []\n",
    "for i, llm_output in enumerate(outputs):\n",
    "    gold = parse(f\"${answers[i]}$\", extraction_config=extraction_target)\n",
    "    answer = parse(llm_output, extraction_config=extraction_target)\n",
    "    result = verify(gold, answer)\n",
    "    results.append(result)\n",
    "accuracy = sum(results) / len(results)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "851c95b1-e050-438f-b8ba-7b0f1908de12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  1460.1266110689917\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\"\n",
    ")\n",
    "length = 0\n",
    "for i in range(len(outputs)):\n",
    "    length += len(tokenizer.tokenize(outputs[i], add_special_tokens=True))\n",
    "print(\"Length: \", length/len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cde653-c05f-4da5-b021-e5a8281bb3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
